# Part II Central Machine Learning Problems 249

## 8 When Models Meet Data 251

### 8.1 Data, Models, and Learning 251

### 8.2 Empirical Risk Minimization 258

### 8.3 Parameter Estimation 265

### 8.4 Probabilistic Modeling and Inference 272

### 8.5 Directed Graphical Models 278

### 8.6 Model Selection 283

## 9 Linear Regression 289

### 9.1 Problem Formulation 291

### 9.2 Parameter Estimation 292

### 9.3 Bayesian Linear Regression 303

### 9.4 Maximum Likelihood as Orthogonal Projection 313

### 9.5 Further Reading 315

## 10 Dimensionality Reduction with Principal Component Analysis 317

### 10.1 Problem Setting 318

### 10.2 Maximum Variance Perspective 320

### 10.3 Projection Perspective 325

### 10.4 Eigenvector Computation and Low-Rank Approximations 333

### 10.5 PCA in High Dimensions 335

### 10.6 Key Steps of PCA in Practice 336

### 10.7 Latent Variable Perspective 339

### 10.8 Further Reading 343

## 11 Density Estimation with Gaussian Mixture Models 348

### 11.1 Gaussian Mixture Model 349

### 11.2 Parameter Learning via Maximum Likelihood 350

### 11.3 EM Algorithm 360

### 11.4 Latent-Variable Perspective 363

### 11.5 Further Reading 368

## 12 Classification with Support Vector Machines 370

### 12.1 Separating Hyperplanes 372

### 12.2 Primal Support Vector Machine 374

### 12.3 Dual Support Vector Machine 383

### 12.4 Kernels 388

### 12.5 Numerical Solution 390

### 12.6 Further Reading 392
